import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
# import requests
# import numpy as np
# from langchain.document_loaders import TextLoader, PyMuPDFLoader, UnstructuredPowerPointLoader, UnstructuredWordDocumentLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
# from langchain.vectorstores import FAISS, Chroma
# from langchain.tools import Tool



# ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ 
model_name = "./DeepSeek-R1-Distill-Qwen-14B"

def load_tokenizer():
    return AutoTokenizer.from_pretrained(model_name)

def load_model():
    return AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True
    )



# ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def generate_answer(prompt):
    # ì‚¬ìš©ì ì…ë ¥ì„ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    st.session_state["messages"].append({"role": "user", "content": user_input})

    # ìµœê·¼ ëŒ€í™” 2ê°œë¥¼ ëª¨ì•„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    chat_history = st.session_state["messages"][-2:]
    prompt = ""
    for msg in chat_history:
        role = msg["role"]
        content = msg["content"]
        prompt += f"{'User' if role == 'user' else 'Assistant'}: {content}\n"
    prompt += "Assistant:"

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € GPUì— ê³„ì† ìœ ì§€
    model = st.session_state["model"]
    tokenizer = st.session_state["tokenizer"]

    # ì±—ë´‡ ì‘ë‹µ
    with torch.no_grad():
        inputs = st.session_state["tokenizer"](prompt, return_tensors="pt").to("cuda")
        outputs = st.session_state["model"].generate(**inputs, max_new_tokens=500)
        response = st.session_state["tokenizer"].decode(outputs[0], skip_special_tokens=True)

    st.session_state["messages"].append({"role": "assistant", "content": response})
    
    return response
if "model" not in st.session_state:
    st.session_state["model"] = load_model().to("cuda")
    st.session_state["tokenizer"] = load_tokenizer()

# Streamlit UI ì„¤ì •
st.title("ğŸ¤– DeepSeek R1 ì±—ë´‡")
st.write("DeepSeek AI ëª¨ë¸ê³¼ ëŒ€í™”í•˜ì„¸ìš”!")

# ì±„íŒ… ê¸°ë¡ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


# ë¬¸ì„œ ì—…ë¡œë“œ

# ë²¡í„° DB ì €ì¥ - LAG

# FAST API ì ìš©



# streamlit UI íŒŒíŠ¸íŠ¸
# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state["messages"].append({"role": "user", "content": user_input})
    
    # UIì— ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(user_input)

    # DeepSeek ëª¨ë¸ ì§ì ‘ í˜¸ì¶œ
    try:
        bot_reply = generate_answer(user_input)
    except Exception as e:
        bot_reply = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    # ì±—ë´‡ ì‘ë‹µ ì €ì¥ ë° í‘œì‹œ
    st.session_state["messages"].append({"role": "assistant", "content": bot_reply})

    with st.chat_message("assistant"):
        st.markdown(bot_reply)

